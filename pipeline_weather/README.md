ğŸŒ¦ï¸ Pipeline de Dados MeteorolÃ³gicos (ETL)

Este projeto demonstra um fluxo completo de ETL (Extract, Transform, Load) orquestrado pelo Apache Airflow, utilizando a API da OpenWeatherMap para coletar dados meteorolÃ³gicos e armazenÃ¡-los em um banco de dados relacional Postgres.

Toda a infraestrutura Ã© provisionada via Docker, garantindo isolamento e facilidade na replicaÃ§Ã£o do ambiente.

ğŸš€ Tecnologias Utilizadas
Linguagem: Python 3.x

OrquestraÃ§Ã£o: Apache Airflow

Banco de Dados: PostgreSQL 16

Gerenciamento de Banco: pgAdmin 4

Infraestrutura: Docker & Docker Compose

API de Dados: OpenWeatherMap

ğŸ—ï¸ Arquitetura do Projeto
O pipeline foi modularizado seguindo as melhores prÃ¡ticas de engenharia, separando as responsabilidades em scripts Python distintos:

extract_data.py: ResponsÃ¡vel por realizar as requisiÃ§Ãµes HTTP para a API, autenticar e obter os dados brutos em formato JSON.

transform_data.py: Processa os dados brutos, realiza limpeza, normalizaÃ§Ã£o e prepara o schema final para o banco de dados.

load_data.py: Gerencia a conexÃ£o com o banco Postgres e realiza a inserÃ§Ã£o (ou atualizaÃ§Ã£o) dos dados transformados.

Airflow DAG: O maestro do projeto, que define a ordem de execuÃ§Ã£o, gerencia retentativas em caso de falha e monitora o status de cada etapa.

ğŸ› ï¸ Como Executar o Projeto
PrÃ©-requisitos
Docker e Docker Compose instalados.

Uma chave de API (API Key) do OpenWeatherMap.

Passo a Passo
Clonar o repositÃ³rio:

Bash
git clone <repositorio>.git
cd <caminho>
Configurar as credenciais:

Crie um arquivo .env ou configure as variÃ¡veis de ambiente no docker-compose.yaml com suas credenciais do Postgres e sua API Key.
API_KEY
database
user
password

Subir os containers:

Bash
docker-compose up -d
Acessar as interfaces:

Airflow: http://localhost:8080 (User: airflow / Pass: airflow)

pgAdmin: http://localhost:5050 (User: admin@admin.com / Pass: admin)

ğŸ“Š Estrutura de Pastas
Plaintext
pipeline_weather/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ weather_dag.py        # DefiniÃ§Ã£o da DAG
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_data.py       # ExtraÃ§Ã£o da API
â”‚   â”œâ”€â”€ transform_data.py     # TransformaÃ§Ã£o dos dados
â”‚   â””â”€â”€ load_data.py          # Carga no Postgres
â”œâ”€â”€ docker-compose.yaml       # ConfiguraÃ§Ã£o da infraestrutura
â””â”€â”€ README.md                 # DocumentaÃ§Ã£o

ğŸ“Œ PrÃ³ximos Passos
[ ] Implementar notificaÃ§Ãµes de falha via Slack/E-mail.

[ ] Criar um dashboard no Power BI ou Metabase conectado ao Postgres.

[ ] Adicionar testes unitÃ¡rios para a etapa de transformaÃ§Ã£o.
