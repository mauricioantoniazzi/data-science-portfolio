# ðŸ›’ ETL Pipeline: E-commerce Analytics (Olist Dataset)

Este projeto demonstra um pipeline de dados de nÃ­vel intermediÃ¡rio utilizando **Apache Airflow**, **Python (Pandas)** e **PostgreSQL**. O objetivo Ã© processar dados brutos de e-commerce, aplicando transformaÃ§Ãµes de negÃ³cio e organizando-os em uma estrutura de banco de dados para anÃ¡lise.

## ðŸ—ï¸ Arquitetura e Fluxo de Dados

O projeto segue a arquitetura de medalhÃ£o para garantir a organizaÃ§Ã£o e qualidade dos dados:
1.  **Extract (Bronze):** Coleta de CSVs brutos e conversÃ£o para formato Parquet (altamente performÃ¡tico).
2.  **Transform (Silver/Gold):** Limpeza de dados, conversÃ£o de tipos (Datetime), Joins complexos e criaÃ§Ã£o de mÃ©tricas de faturamento.
3.  **Load:** Carga automatizada no PostgreSQL utilizando SQLAlchemy.

## ðŸ› ï¸ Tecnologias Utilizadas
* **OrquestraÃ§Ã£o:** Apache Airflow 3.x (TaskFlow API)
* **Processamento:** Python 3.12, Pandas, PyArrow
* **Banco de Dados:** PostgreSQL & pgAdmin 4
* **Infraestrutura:** Docker & Docker Compose
* **Gerenciamento de Pacotes:** `uv`

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### 1. VariÃ¡veis de Ambiente
Para que o pipeline conecte corretamente ao banco de dados, vocÃª deve configurar as credenciais. 

> **Importante:** Configure o arquivo na pasta `config/.env` com os campos de banco de dados, usuÃ¡rio e senha.

Exemplo de conteÃºdo para o `config/.env`:
```env
database=db_ecommerce
user=airflow
password=airflow
```
### 2. InicializaÃ§Ã£o com Docker
No diretÃ³rio raiz, execute os comandos abaixo para configurar o UID do Airflow e subir os serviÃ§os:

```
echo "AIRFLOW_UID=$(id -u)" > .env
docker-compose up -d
```

ðŸ“‚ Estrutura do RepositÃ³rio
dags/: DefiniÃ§Ã£o da DAG orquestradora.

src/: Scripts modulares de ETL (extract.py, transform.py, load.py).

data/: Volumes para persistÃªncia de arquivos (Raw, Processing, Gold).

config/: Arquivos de configuraÃ§Ã£o e ambiente.

notebooks/: ExploraÃ§Ã£o inicial e validaÃ§Ã£o dos dados.

ðŸš€ Como Executar
Certifique-se de que os arquivos CSV do Olist estÃ£o em data/bronze/.

Acesse o Airflow UI em http://localhost:8081.

Ative a DAG etl_ecommerce_olist_v1.

Monitore a execuÃ§Ã£o e verifique os dados finais no pgAdmin ou via Notebook.
